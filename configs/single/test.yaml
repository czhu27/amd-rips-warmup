batch_size: 1024
layers: [3, 100, 100, 100, 100, 100, 3]
lr: 1.0e-3
grad_reg_const: 0.1    # !!!
activation: tanh
model_outputs: all
epochs: 10

gradient_loss: 
  - region: all   # which data to apply to
    name: velocity_lr     # id of gradient regularizer
    weight: [0.0, 0.03]    # start_weight, end_weight
    schedule: [200, 500]  # start_epoch,  end_epoch
  - region: all   
    name: velocity_ud     
    weight: [0.0, 0.03]    
    schedule: [200, 500]  
  - region: interior
    name: first_explicit     
    weight: [0.0, 0.03]    
    schedule: [600, 1000]  
  - region: interior
    name: second_explicit     
    weight: [0.0, 0.01]    
    schedule: [1100, 1400]  
  - region: interior
    name: second_curl     
    weight: [0.0, 0.01]    
    schedule: [1500, 1800] 
lr_scheduler: true
# lr_scheduler_type: polynomial_decay
# lr_scheduler_params: [1.0e-3, 3.0e-5, 10, 1.0] #[initial, final, decay_steps, power]
lr_scheduler_type: piecewise_linear
lr_scheduler_params: [1.0e-3, 1.0e-4, 2000, 2400] #[initial, final, start_epoch, end_epoch]

loss_schedulerizer: false
loss_schedulerizer_params: [500, 1000] #[Begin adding grad, fully added]

data_run: test

# false -> slow graph, fast epoch
# true -> fast graph, slower epoch
from_tensor_slices: true
shuffle: true # Shuffles at the start of each epoch

# turns on eagerly mode
debug: false
tb_error_timestep: 5
