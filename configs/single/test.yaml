batch_size: 512
layers: [3, 100, 100, 100, 100, 100, 3]
lr: 1.0e-3
grad_reg_const: 0.01     # !!!
activation: tanh
model_outputs: all
gradient_loss: none
epochs: 5000
lr_scheduler: true
lr_scheduler_params: [1.0e-3, 1.0e-5, 500, 1.0] #[initial, final, decay_steps, power]

from_tensor_slices: true