#############
#  Dataset  #
#############
source: synthetic
target: parabola
target_coefficients: [1.0, 1.0]

corners: false
dataset: [2500, 7500, 0, 1.0, 0.0] #[interior, exterior, border, labeled_int%, labeled_ext%]
noise: 0.0

###########
#  Model  #
###########
activation: swish
dropout_rates: 0.0
layers: [2, 30, 30, 30, 30, 1]

##############
#  Training  #
##############
device: gpu
trials: 1
seed: 0
epochs: 300
batch_size: 256

lr: 1.0e-4
lr_scheduler: true
lr_scheduler_type: piecewise_linear
lr_scheduler_params: [1.0e-3, 1.0e-6, 100, 200] #[initial, final, decay_steps, power]

gd_noise: 0.0
from_tensor_slices: true
shuffle: true

##################
#  Regularizers  #
##################
regularizer: none
reg_const: 0.1
gradient_loss:
  - region: all             # which data to apply to
    name: second            # id of gradient regularizer
    weight: 1               # start_weight, end_weight
grad_reg_const: 1
loss_schedulerizer: false
# loss_schedulerizer_params: [2000, 2400] #[Begin adding grad, fully added]

#############
#  Logging  #
#############
debug: false
output_dir: null   # overridden in main.py
output_root: output/toy
plots: [extrapolation, data-distribution, tensorboard]
saves: [model]
tb_error_timestep: 20
tb_loss_timestep: 5
