independent_vars:
  grad_reg_const: [0.01, 0.001, 0.0001]

batch_size: 512
layers: [3, 100, 100, 100, 100, 100, 3]
lr: 1.0e-3
grad_reg_const: 0.0001    # !!!
activation: tanh
model_outputs: all
gradient_loss: 
  wave_eq: [first_explicit]
  boundary: [velocity]
epochs: 1000

lr_scheduler: true
#lr_scheduler_type: polynomial_decay
#lr_scheduler_params: [1.0e-3, 3.0e-5, 10, 1.0] #[initial, final, decay_steps, power]
lr_scheduler_type: piecewise_linear
lr_scheduler_params: [1.0e-3, 5.0e-5, 250, 750] #[initial, final, start_epoch, end_epoch]

loss_schedulerizer: true
loss_schedulerizer_params: [250, 750] #[Begin adding grad, fully added]

data_run: fine_mesh_gauss_few_bound 

# false -> slow graph, fast epoch
# true -> fast graph, slower epoch
from_tensor_slices: false
shuffle: true # Shuffles at the start of each epoch

# turns on eagerly mode
debug: false
