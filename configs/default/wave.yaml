###########
#  Model  #
###########
activation: tanh
dropout_rates: 0.0
layers: [3, 100, 100, 100, 100, 100, 3]

##############
#  Training  #
##############
device: gpu
trials: 1
seed: 0
epochs: 2500
batch_size: 512

lr: 1.0e-3
lr_scheduler: true
#lr_scheduler_type: polynomial_decay
#lr_scheduler_params: [1.0e-3, 3.0e-5, 10, 1.0] #[initial, final, decay_steps, power]
lr_scheduler_type: piecewise_linear
lr_scheduler_params: [1.0e-3, 5.0e-5, 500, 2000] #[initial, final, start_epoch, end_epoch]

gd_noise: 0.0

#############
#  Dataset  #
#############
#source: wave_with_source
source: wave
data_dir: data/wave/
data_run: [fine_mesh_gauss_few] 

from_tensor_slices: true

# make sure data_sources correspond with files in data_run
# also make sure data_sources is list of points
#data_sources: [[0.5, 0.5], [0.9, 0.1], [0.25, 0.75]]
#data_run: [05-05, 09-01, 025-075]

# test_source and test_data_dir only matter for error calculation and prediction plots
test_source: [0.6, 0.6]
test_data_dir: data/wave/06-06/
shuffle: true

##################
#  Regularizers  #
##################
regularizer: none
reg_const: 0.1
# none, const, zero, first
model_outputs: pressure
gradient_loss: 
  wave_eq: []
  boundary: []
loss_schedulerizer: true
loss_schedulerizer_params: [500, 2000] #[Begin adding grad, fully added]
#grad_reg_norm: l1 l2
grad_reg_const: 1
# grad_reg_scheduler: late_start
# grad_reg_start: 

#############
#  Logging  #
#############
debug: false
output_dir: null   # overridden in main.py
output_root: output/wave
plots: [extrapolation, data-distribution, tensorboard, extrapolation_wave, heatmap]
saves: [model]
tb_error_timestep: 100
