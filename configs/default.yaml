###########
#  Model  #
###########
activation: elu
layers: [2, 20, 20, 20, 1]

##############
#  Training  #
##############
epochs: 1000
batch_size: 256
lr: 1.0e-4
lr_scheduler: false
lr_scheduler_params: [1.0e-3, 1.0e-6, 2500, 1.0] #[initial, final, decay_steps, power]

#############
#  Dataset  #
#############
output_dir: null   # overridden in main.py
f_a: 1.0
f_b: 1.0
target: parabola
corners: false
<<<<<<< HEAD
dataset: [.25, .75, 1.0, 0.0] #[interior%, exterior%, labeled_int%, labeled_ext%]
=======
dataset: [10000, 0, 30000, 1.0, 0.0] #[interior, exterior, border, labeled_int%, labeled_ext%]
noise: 0.01
>>>>>>> 02a01f0fb89b442d3f8b4aeaaeda22bd53dcde75

##################
#  Regularizers  #
##################
regularizer: none
reg_const: 0.1
gradient_loss: const
#grad_reg_norm: l1 l2
grad_reg_const: 1

#############
#  Logging  #
#############
debug: false
plots: true
detailed_save: true

dropout_rates: [0.0, 0.0, 0.0]